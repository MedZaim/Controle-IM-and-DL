{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-14T16:19:50.274262700Z",
     "start_time": "2026-01-14T16:19:50.253754300Z"
    }
   },
   "source": [
    "#1.Imports and Reproducibility\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T16:19:50.306190300Z",
     "start_time": "2026-01-14T16:19:50.287943800Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "527c45afd1b8218f",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T16:19:50.336099500Z",
     "start_time": "2026-01-14T16:19:50.318419700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#2.Custom Dataset (Using New Binary CSV)\n",
    "class HAM500Binary(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)  # standard comma-separated CSV\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.iloc[idx]['image_id']\n",
    "        label = int(self.df.iloc[idx]['binary_label'])  # already 0 or 1\n",
    "\n",
    "        # Try common image extensions\n",
    "        for ext in ['.jpg', '.png', '.jpeg']:\n",
    "            path = os.path.join(self.img_dir, img_id + ext)\n",
    "            if os.path.exists(path):\n",
    "                image = Image.open(path).convert('RGB')\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_id}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ],
   "id": "bf4536d4b200892e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T16:19:50.401366900Z",
     "start_time": "2026-01-14T16:19:50.346426500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#3.Data Loading and 80/20 Split\n",
    "\n",
    "# Define transform (64x64, normalized)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load dataset using new binary CSV\n",
    "dataset = HAM500Binary(\n",
    "    csv_path='../HAM5000/HAM500_metadata_binary.csv',\n",
    "    img_dir='../HAM5000/HAM500_images',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "# Fixed 80/20 split with seed\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "n_train = int(0.8 * len(dataset))\n",
    "train_indices = indices[:n_train]\n",
    "val_indices = indices[n_train:]\n",
    "\n",
    "train_set = Subset(dataset, train_indices)\n",
    "val_set = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_set)} samples | Val: {len(val_set)} samples\")"
   ],
   "id": "5f4a4e107a622f56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 400 samples | Val: 100 samples\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T16:19:57.318888700Z",
     "start_time": "2026-01-14T16:19:50.404368800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Full dataset statistics\n",
    "all_labels = [dataset[i][1] for i in range(len(dataset))]\n",
    "n_malade_total = sum(all_labels)\n",
    "n_saine_total = len(all_labels) - n_malade_total\n",
    "print(\"=== FULL DATASET ===\")\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "print(f\"Malade (mel): {n_malade_total} ({n_malade_total / len(dataset) * 100:.1f}%)\")\n",
    "print(f\"Saine (others): {n_saine_total} ({n_saine_total / len(dataset) * 100:.1f}%)\\n\")\n",
    "\n",
    "# Training set statistics\n",
    "train_labels = [dataset[i][1] for i in train_indices]\n",
    "n_malade_train = sum(train_labels)\n",
    "n_saine_train = len(train_labels) - n_malade_train\n",
    "print(\"=== TRAINING SET ===\")\n",
    "print(f\"Total samples: {len(train_set)}\")\n",
    "print(f\"Malade (mel): {n_malade_train} ({n_malade_train / len(train_set) * 100:.1f}%)\")\n",
    "print(f\"Saine (others): {n_saine_train} ({n_saine_train / len(train_set) * 100:.1f}%)\\n\")\n",
    "\n",
    "# Validation set statistics\n",
    "val_labels = [dataset[i][1] for i in val_indices]\n",
    "n_malade_val = sum(val_labels)\n",
    "n_saine_val = len(val_labels) - n_malade_val\n",
    "print(\"=== VALIDATION SET ===\")\n",
    "print(f\"Total samples: {len(val_set)}\")\n",
    "print(f\"Malade (mel): {n_malade_val} ({n_malade_val / len(val_set) * 100:.1f}%)\")\n",
    "print(f\"Saine (others): {n_saine_val} ({n_saine_val / len(val_set) * 100:.1f}%)\")"
   ],
   "id": "65e999ba6508f5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FULL DATASET ===\n",
      "Total samples: 500\n",
      "Malade (mel): 43 (8.6%)\n",
      "Saine (others): 457 (91.4%)\n",
      "\n",
      "=== TRAINING SET ===\n",
      "Total samples: 400\n",
      "Malade (mel): 35 (8.8%)\n",
      "Saine (others): 365 (91.2%)\n",
      "\n",
      "=== VALIDATION SET ===\n",
      "Total samples: 100\n",
      "Malade (mel): 8 (8.0%)\n",
      "Saine (others): 92 (92.0%)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T16:19:57.343199Z",
     "start_time": "2026-01-14T16:19:57.330716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#4.Common Classification Head (VGG16 Head)\n",
    "class VGG16Head(nn.Module):\n",
    "    def __init__(self, input_dim=512, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_dim, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)"
   ],
   "id": "8668d57cf6186189",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T16:19:57.358180600Z",
     "start_time": "2026-01-14T16:19:57.345571900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#5. Model 1 – Custom CNN (From Scratch)\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # Block 4\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # Block 5: 1x1 conv + GAP\n",
    "            nn.Conv2d(256, 512, 1), nn.ReLU(),\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.head = VGG16Head(input_dim=512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x).view(x.size(0), -1)\n",
    "        return self.head(x)"
   ],
   "id": "a950e683b393c0a2",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T16:19:57.373429900Z",
     "start_time": "2026-01-14T16:19:57.360617100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#6. Model 2 – VGG16 (From Scratch)\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.head = VGG16Head(input_dim=512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x).view(x.size(0), -1)\n",
    "        return self.head(x)"
   ],
   "id": "1efa7aa257e4fb",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T16:19:57.392032400Z",
     "start_time": "2026-01-14T16:19:57.376572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#7. Models 3 & 4 – Vision Transformer (ViT-1 and ViT-2)\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=64, patch_size=8, in_chans=3, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class ViTEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=64, n_heads=4, mlp_ratio=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        hidden_dim = embed_dim * mlp_ratio\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, n_blocks=1, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(embed_dim=64)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, 64))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 65, 64))  # 64 patches + 1 cls token\n",
    "        self.encoder = nn.Sequential(*[ViTEncoderBlock() for _ in range(n_blocks)])\n",
    "        self.to_512 = nn.Linear(64, 512)\n",
    "        self.head = VGG16Head(input_dim=512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.encoder(x)\n",
    "        features = self.to_512(x[:, 0])  # Use [CLS] token\n",
    "        return self.head(features)"
   ],
   "id": "6b0e98cc75cae362",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T16:19:57.409461500Z",
     "start_time": "2026-01-14T16:19:57.394068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#8. Training and Evaluation Function\n",
    "def train_and_evaluate(model, train_loader, val_loader, epochs=8, lr=1e-3, model_name=\"Model\"):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        history['train_loss'].append(total_train_loss / len(train_loader))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Metrics (binary classification)\n",
    "        all_labels = np.array(all_labels, dtype=int)\n",
    "        all_preds = np.array(all_preds, dtype=int)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        prec = precision_score(all_labels, all_preds, zero_division=0, average='binary')\n",
    "        rec = recall_score(all_labels, all_preds, zero_division=0, average='binary')\n",
    "        f1 = f1_score(all_labels, all_preds, zero_division=0, average='binary')\n",
    "\n",
    "        history['val_loss'].append(total_val_loss / len(val_loader))\n",
    "        history['val_acc'].append(acc)\n",
    "        history['val_f1'].append(f1)\n",
    "\n",
    "        print(f\"[{model_name}] Epoch {epoch+1}/{epochs} | Acc: {acc:.4f} | Recall (Malade): {rec:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "    # Save curves\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['val_acc'], label='Accuracy')\n",
    "    plt.plot(history['val_f1'], label='F1-score')\n",
    "    plt.title(f'{model_name} - Metrics')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'courbes_{model_name.replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}"
   ],
   "id": "e9f24c178635efcc",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T16:29:08.787250200Z",
     "start_time": "2026-01-14T16:19:57.412597100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#9. Train All Four Models\n",
    "\n",
    "models = {\n",
    "    \"CustomCNN\": CustomCNN(),\n",
    "    \"VGG16\": VGG16(),\n",
    "    \"ViT-1\": ViT(n_blocks=1),\n",
    "    \"ViT-2\": ViT(n_blocks=2)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    results[name] = train_and_evaluate(model, train_loader, val_loader, epochs=8, model_name=name)"
   ],
   "id": "83b5800bfc867df5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training CustomCNN ---\n",
      "[CustomCNN] Epoch 1/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[CustomCNN] Epoch 2/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[CustomCNN] Epoch 3/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[CustomCNN] Epoch 4/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[CustomCNN] Epoch 5/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[CustomCNN] Epoch 6/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[CustomCNN] Epoch 7/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[CustomCNN] Epoch 8/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "\n",
      "--- Training VGG16 ---\n",
      "[VGG16] Epoch 1/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[VGG16] Epoch 2/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[VGG16] Epoch 3/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[VGG16] Epoch 4/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[VGG16] Epoch 5/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[VGG16] Epoch 6/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[VGG16] Epoch 7/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[VGG16] Epoch 8/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "\n",
      "--- Training ViT-1 ---\n",
      "[ViT-1] Epoch 1/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-1] Epoch 2/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-1] Epoch 3/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-1] Epoch 4/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-1] Epoch 5/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-1] Epoch 6/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-1] Epoch 7/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-1] Epoch 8/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "\n",
      "--- Training ViT-2 ---\n",
      "[ViT-2] Epoch 1/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-2] Epoch 2/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-2] Epoch 3/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-2] Epoch 4/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-2] Epoch 5/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-2] Epoch 6/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-2] Epoch 7/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n",
      "[ViT-2] Epoch 8/8 | Acc: 0.9200 | Recall (Malade): 0.0000 | F1: 0.0000\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T16:29:08.824547900Z",
     "start_time": "2026-01-14T16:29:08.802014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#10: Print Final Results Table (For Word Report)\n",
    "\n",
    "print(\"\\nFINAL VALIDATION METRICS (AFTER 8 EPOCHS):\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Model':<12} | {'Accuracy':<10} | {'Precision':<10} | {'Recall (Malade)':<15} | {'F1-score':<10}\")\n",
    "print(\"-\" * 75)\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name:<12} | {metrics['accuracy']:<10.4f} | {metrics['precision']:<10.4f} | {metrics['recall']:<15.4f} | {metrics['f1']:<10.4f}\")"
   ],
   "id": "6175fb5972adc1fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL VALIDATION METRICS (AFTER 8 EPOCHS):\n",
      "---------------------------------------------------------------------------\n",
      "Model        | Accuracy   | Precision  | Recall (Malade) | F1-score  \n",
      "---------------------------------------------------------------------------\n",
      "CustomCNN    | 0.9200     | 0.0000     | 0.0000          | 0.0000    \n",
      "VGG16        | 0.9200     | 0.0000     | 0.0000          | 0.0000    \n",
      "ViT-1        | 0.9200     | 0.0000     | 0.0000          | 0.0000    \n",
      "ViT-2        | 0.9200     | 0.0000     | 0.0000          | 0.0000    \n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T16:29:09.354135800Z",
     "start_time": "2026-01-14T16:29:08.825599400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sauvegarder chaque modèle après entraînement\n",
    "\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "for name, model in models.items():\n",
    "    torch.save(model.state_dict(), f\"models/{name}.pth\")\n",
    "    print(f\"\\n--- the model '{name}' saved in 'models/{name}.pth' successfully ---\")\n",
    "\n"
   ],
   "id": "7d28122c2f7743a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- the model 'CustomCNN' saved in 'models/CustomCNN.pth' successfully ---\n",
      "\n",
      "--- the model 'VGG16' saved in 'models/VGG16.pth' successfully ---\n",
      "\n",
      "--- the model 'ViT-1' saved in 'models/ViT-1.pth' successfully ---\n",
      "\n",
      "--- the model 'ViT-2' saved in 'models/ViT-2.pth' successfully ---\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
